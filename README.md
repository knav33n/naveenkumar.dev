# My Journey through - The Cloud Resume Challenge

Links
- Site: [https://naveenkumar.dev](https://naveenkumar.dev)
- Repo: [https://github.com/knav33n/naveenkumar.dev](https://github.com/knav33n/naveenkumar.dev)

As someone who loves to learn by doing, I was actively exploring ways to gain practical experience in cloud-related skills. That's when I stumbled upon The Cloud Resume Challenge. Even though I have prior experience in software development, I was genuinely amazed at how much I learned through this challenge. In a nutshell, the challenge invites you to deploy an application on a cloud provider of your choice. The flexibility to choose any provider intrigued me, and given my familiarity with AWS, I decided to go with it.

Creating the static site was a breeze for me, as I've been working with front-end development since I first started in software. Instead of opting for a comprehensive resume, I decided to go with a simpler introductory/portfolio page. Deploying the static site proved to be relatively straightforward using the user interface. First, I needed to generate a certificate for my domain through ACM (AWS Certificate Manager), which was then validated through a CNAME entry. Next, I set up an S3 bucket and created a CloudFront distribution that linked to the S3 bucket, making sure to include the alternate domain I had purchased. I then updated the bucket policy with the one provided by CloudFront. At this point, I could access the site using the domain generated by CloudFront. To further connect everything together, I created a CNAME entry in Route53 to point my domain to the CloudFront domain. Voila! The site was now fully accessible through my own domain name.

The next step involved writing a lambda function to track the number of views and store them in DynamoDB. This part took some time since I was relatively new to working with Python and it was my first experience with Lambda and DynamoDB. To ensure the lambda function could access the DynamoDB table, I created a relevant role that granted the necessary read and write permissions.

Lambda provided a function URL that could be utilized to call it. Curious to explore further, I decided to experiment by using a subdomain of mine to invoke this lambda function. This required setting up an API Gateway and configuring it to forward requests to the lambda function(though this is not in the final repo). To ensure security, I updated the CORS settings of the lambda function, ensuring that it could only be called from my own domain.

Now, whenever the page loads, an AJAX call from the front-end triggered the update of the DynamoDB table. The response from this update contained the updated view count, which was then displayed on the UI. It was truly rewarding to witness the views being dynamically updated in real time.

The next crucial step was to incorporate monitoring capabilities. To achieve this, I proceeded to set up CloudWatch alarms to keep a close eye on any errors or invocations of the lambda function. In order to receive timely notifications, I configured SNS topics and created an additional Lambda function with the necessary role. This Lambda function was specifically designed to register and publish these alarms to a designated channel within a Slack workspace. By utilizing webhooks, I established a seamless integration between CloudWatch and Slack, enabling real-time updates and notifications.

For source control, I relied on GitHub, although I must admit that GitHub Actions was a new concept for me. I encountered numerous failed workflows along the way, which was a bit frustrating. To make matters more confusing, there were instances when GitHub Actions experienced downtime, adding to the challenges I faced.

Next, I delved into using Terraform to provision the necessary infrastructure. It was a whole new learning experience, and I dedicated hours upon hours to understanding how it functions. I scoured through documentation, learning the ins and outs of deploying the desired resources. It involved a lot of trial and error, and there were countless moments of searching for solutions.

But eventually, after much persistence and effort, I managed to provision everything I needed using Terraform. From setting up S3 buckets to configuring CloudWatch alarms, I successfully achieved the desired infrastructure setup. It was truly fulfilling to see everything fall into place after overcoming the initial learning curve and persisting through the challenges.

In conclusion, this journey has been nothing short of incredible, filled with invaluable learning experiences. I am immensely grateful to Forrest Brazeal for crafting such an amazing challenge. It has pushed me to explore new territories, expand my skill set, and discover the boundless possibilities of cloud-related technologies.

Helpful links -
- https://www.automat-it.com/post/using-github-actions-with-aws-iam-roles
- https://github.com/aws-actions/configure-aws-credentials
- https://repost.aws/knowledge-center/cloudfront-serve-static-website
- https://www.netlify.com/guides/creating-an-api-with-aws-lambda-dynamodb-and-api-gateway/setting-up-our-lambda-function/
- https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/
- https://stackoverflow.com/questions/57288992/terraform-how-to-create-iam-role-for-aws-lambda-and-deploy-both
- https://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html#SSS-CreateFunction-request-Runtime
- https://stackoverflow.com/a/56070283
- https://stackoverflow.com/a/67879838
- https://www.youtube.com/watch?v=NxQgltu1yYM
- https://www.youtube.com/watch?v=G4_ay2_h9GI
- https://www.youtube.com/watch?v=ox_HJ8w7FPI
- https://medium.com/analytics-vidhya/generate-slack-notifications-for-aws-cloudwatch-alarms-e46b68540133
- https://github.com/aws-actions/configure-aws-credentials/issues/357